An end-to-end data engineering pipeline that ingests, cleans, transforms, warehouses, and analyzes 18M+ NYC taxi trip records using Python, Pandas, Parquet, and DuckDB. Designed to handle large-scale datasets, schema drift, and real-world data quality issues.

**Project Overview**
This project demonstrates a production-style ETL pipeline capable of processing millions of records efficiently on a local machine.
**Key features include:**
Chunk-based ingestion to handle very large CSV files
Data cleaning and normalization
Parquet-based data lake storage
DuckDB analytics warehouse
SQL-based analytics and KPI generation
Clean, modular project structure
GitHub-friendly data handling (sample data + Kaggle source)
**What This Project Demonstrates**
Real-world data engineering best practices
Handling large datasets locally
Clean ETL architecture
Data warehouse modeling
Analytical SQL proficiency
Production-ready project structure
